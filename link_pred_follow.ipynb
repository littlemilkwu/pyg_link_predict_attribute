{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyg-team\n",
    "# from: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/link_pred.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as ospath\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.data import Data, Dataset\n",
    "import torch_geometric.transforms as Tr\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix1 = 'dataset1'\n",
    "prefix2 = 'dataset2'\n",
    "prefix3 = 'dataset3'\n",
    "\n",
    "df_train1 = pd.read_csv(f'{prefix1}/train.csv')\n",
    "df_test1 = pd.read_csv(f'{prefix1}/test.csv')\n",
    "df_content1 = pd.read_csv(f'{prefix1}/content.csv', sep='\\t', header=None, index_col=0)\n",
    "\n",
    "df_train2 = pd.read_csv(f'{prefix2}/train.csv')\n",
    "df_test2 = pd.read_csv(f'{prefix2}/test.csv')\n",
    "df_content2 = pd.read_csv(f'{prefix2}/content.csv', sep='\\t', header=None, index_col=0)\n",
    "\n",
    "df_train3 = pd.read_csv(f'{prefix3}/train.csv')\n",
    "df_test3 = pd.read_csv(f'{prefix3}/test.csv')\n",
    "df_content3 = pd.read_csv(f'{prefix3}/content.csv', sep='\\t', header=None, index_col=0)\n",
    "\n",
    "df_content1 = df_content1.sort_index()\n",
    "df_content2 = df_content2.sort_index()\n",
    "df_content3 = df_content3.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape    : (8686, 4)\n",
      "Test Shape     : (2172, 3)\n",
      "Content Shape  : (2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "def print_shape(df_train, df_test, df_content):\n",
    "    print(f\"{'Train Shape':<15}: {df_train.shape}\")\n",
    "    print(f\"{'Test Shape':<15}: {df_test.shape}\")\n",
    "    print(f\"{'Content Shape':<15}: {df_content.shape}\")\n",
    "\n",
    "print_shape(df_train1, df_test1, df_content1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2data(df_train, df_test, df_content):\n",
    "    X = torch.tensor(df_content.values, dtype=torch.float32)\n",
    "    train_edge_index = torch.tensor(df_train.iloc[:, 1:3].values).T\n",
    "    train_edge_label_index = torch.tensor(df_train.loc[(df_train['label'] == 1), ['from', 'to']].values).T\n",
    "    train_edge_label = torch.ones(size=(train_edge_label_index.shape[1], ))\n",
    "\n",
    "    test_edge_label_index = torch.tensor(df_test.iloc[:, 1:3].values).T\n",
    "    return (Data(x=X, edge_index=train_edge_index, edge_label_index=train_edge_label_index, edge_label=train_edge_label), \n",
    "            Data(x=X, edge_index=train_edge_index, edge_label_index=test_edge_label_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1, test_data1 = df2data(df_train1, df_test1, df_content1)\n",
    "train_data2, test_data2 = df2data(df_train2, df_test2, df_content2)\n",
    "train_data3, test_data3 = df2data(df_train3, df_test3, df_content3)\n",
    "\n",
    "# Tr = from torch_geometric\n",
    "transform = Tr.Compose([\n",
    "    Tr.NormalizeFeatures(),\n",
    "    Tr.ToDevice(device),\n",
    "    # Tr.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "    #                    add_negative_train_samples=False)\n",
    "])\n",
    "train_data1, test_data1 = transform(train_data1), transform(test_data1)\n",
    "train_data2, test_data2 = transform(train_data2), transform(test_data2)\n",
    "train_data3, test_data3 = transform(train_data3), transform(test_data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[2708, 1433], edge_index=[2, 8686], edge_label_index=[2, 4324], edge_label=[4324]),\n",
       " Data(x=[2708, 1433], edge_index=[2, 8686], edge_label_index=[2, 2172])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[train_data1, test_data1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Loss: 0.6982\n",
      "Epoch: 0002, Loss: 0.6935\n",
      "Epoch: 0003, Loss: 0.7006\n",
      "Epoch: 0004, Loss: 0.6931\n",
      "Epoch: 0005, Loss: 0.6933\n",
      "Epoch: 0006, Loss: 0.6935\n",
      "Epoch: 0007, Loss: 0.6934\n",
      "Epoch: 0008, Loss: 0.6934\n",
      "Epoch: 0009, Loss: 0.6933\n",
      "Epoch: 0010, Loss: 0.6930\n",
      "Epoch: 0011, Loss: 0.6923\n",
      "Epoch: 0012, Loss: 0.6915\n",
      "Epoch: 0013, Loss: 0.6897\n",
      "Epoch: 0014, Loss: 0.6868\n",
      "Epoch: 0015, Loss: 0.6814\n",
      "Epoch: 0016, Loss: 0.6740\n",
      "Epoch: 0017, Loss: 0.6675\n",
      "Epoch: 0018, Loss: 0.6594\n",
      "Epoch: 0019, Loss: 0.6476\n",
      "Epoch: 0020, Loss: 0.7020\n",
      "Epoch: 0021, Loss: 0.6789\n",
      "Epoch: 0022, Loss: 0.6842\n",
      "Epoch: 0023, Loss: 0.6652\n",
      "Epoch: 0024, Loss: 0.6477\n",
      "Epoch: 0025, Loss: 0.6532\n",
      "Epoch: 0026, Loss: 0.6673\n",
      "Epoch: 0027, Loss: 0.6668\n",
      "Epoch: 0028, Loss: 0.6546\n",
      "Epoch: 0029, Loss: 0.6520\n",
      "Epoch: 0030, Loss: 0.6522\n",
      "Epoch: 0031, Loss: 0.6528\n",
      "Epoch: 0032, Loss: 0.6438\n",
      "Epoch: 0033, Loss: 0.6371\n",
      "Epoch: 0034, Loss: 0.6303\n",
      "Epoch: 0035, Loss: 0.6336\n",
      "Epoch: 0036, Loss: 0.6304\n",
      "Epoch: 0037, Loss: 0.6284\n",
      "Epoch: 0038, Loss: 0.6265\n",
      "Epoch: 0039, Loss: 0.6318\n",
      "Epoch: 0040, Loss: 0.6156\n",
      "Epoch: 0041, Loss: 0.6233\n",
      "Epoch: 0042, Loss: 0.6132\n",
      "Epoch: 0043, Loss: 0.6150\n",
      "Epoch: 0044, Loss: 0.6201\n",
      "Epoch: 0045, Loss: 0.6174\n",
      "Epoch: 0046, Loss: 0.6096\n",
      "Epoch: 0047, Loss: 0.6086\n",
      "Epoch: 0048, Loss: 0.6110\n",
      "Epoch: 0049, Loss: 0.6080\n",
      "Epoch: 0050, Loss: 0.6107\n",
      "Epoch: 0051, Loss: 0.6064\n",
      "Epoch: 0052, Loss: 0.6091\n",
      "Epoch: 0053, Loss: 0.6036\n",
      "Epoch: 0054, Loss: 0.6072\n",
      "Epoch: 0055, Loss: 0.6007\n",
      "Epoch: 0056, Loss: 0.6021\n",
      "Epoch: 0057, Loss: 0.5983\n",
      "Epoch: 0058, Loss: 0.5920\n",
      "Epoch: 0059, Loss: 0.5959\n",
      "Epoch: 0060, Loss: 0.5919\n",
      "Epoch: 0061, Loss: 0.5931\n",
      "Epoch: 0062, Loss: 0.5959\n",
      "Epoch: 0063, Loss: 0.5925\n",
      "Epoch: 0064, Loss: 0.5910\n",
      "Epoch: 0065, Loss: 0.5905\n",
      "Epoch: 0066, Loss: 0.5832\n",
      "Epoch: 0067, Loss: 0.5839\n",
      "Epoch: 0068, Loss: 0.5864\n",
      "Epoch: 0069, Loss: 0.5886\n",
      "Epoch: 0070, Loss: 0.5930\n",
      "Epoch: 0071, Loss: 0.5848\n",
      "Epoch: 0072, Loss: 0.5827\n",
      "Epoch: 0073, Loss: 0.5821\n",
      "Epoch: 0074, Loss: 0.5872\n",
      "Epoch: 0075, Loss: 0.5782\n",
      "Epoch: 0076, Loss: 0.5747\n",
      "Epoch: 0077, Loss: 0.5807\n",
      "Epoch: 0078, Loss: 0.5896\n",
      "Epoch: 0079, Loss: 0.5807\n",
      "Epoch: 0080, Loss: 0.5669\n",
      "Epoch: 0081, Loss: 0.5821\n",
      "Epoch: 0082, Loss: 0.5764\n",
      "Epoch: 0083, Loss: 0.5797\n",
      "Epoch: 0084, Loss: 0.5766\n",
      "Epoch: 0085, Loss: 0.5775\n",
      "Epoch: 0086, Loss: 0.5678\n",
      "Epoch: 0087, Loss: 0.5760\n",
      "Epoch: 0088, Loss: 0.5671\n",
      "Epoch: 0089, Loss: 0.5757\n",
      "Epoch: 0090, Loss: 0.5715\n",
      "Epoch: 0091, Loss: 0.5703\n",
      "Epoch: 0092, Loss: 0.5674\n",
      "Epoch: 0093, Loss: 0.5670\n",
      "Epoch: 0094, Loss: 0.5755\n",
      "Epoch: 0095, Loss: 0.5697\n",
      "Epoch: 0096, Loss: 0.5697\n",
      "Epoch: 0097, Loss: 0.5731\n",
      "Epoch: 0098, Loss: 0.5633\n",
      "Epoch: 0099, Loss: 0.5691\n",
      "Epoch: 0100, Loss: 0.5720\n",
      "Epoch: 0101, Loss: 0.5704\n",
      "Epoch: 0102, Loss: 0.5689\n",
      "Epoch: 0103, Loss: 0.5653\n",
      "Epoch: 0104, Loss: 0.5736\n",
      "Epoch: 0105, Loss: 0.5698\n",
      "Epoch: 0106, Loss: 0.5666\n",
      "Epoch: 0107, Loss: 0.5697\n",
      "Epoch: 0108, Loss: 0.5617\n",
      "Epoch: 0109, Loss: 0.5674\n",
      "Epoch: 0110, Loss: 0.5710\n",
      "Epoch: 0111, Loss: 0.5666\n",
      "Epoch: 0112, Loss: 0.5629\n",
      "Epoch: 0113, Loss: 0.5692\n",
      "Epoch: 0114, Loss: 0.5626\n",
      "Epoch: 0115, Loss: 0.5676\n",
      "Epoch: 0116, Loss: 0.5668\n",
      "Epoch: 0117, Loss: 0.5658\n",
      "Epoch: 0118, Loss: 0.5584\n",
      "Epoch: 0119, Loss: 0.5590\n",
      "Epoch: 0120, Loss: 0.5556\n",
      "Epoch: 0121, Loss: 0.5600\n",
      "Epoch: 0122, Loss: 0.5598\n",
      "Epoch: 0123, Loss: 0.5649\n",
      "Epoch: 0124, Loss: 0.5682\n",
      "Epoch: 0125, Loss: 0.5611\n",
      "Epoch: 0126, Loss: 0.5625\n",
      "Epoch: 0127, Loss: 0.5581\n",
      "Epoch: 0128, Loss: 0.5529\n",
      "Epoch: 0129, Loss: 0.5640\n",
      "Epoch: 0130, Loss: 0.5579\n",
      "Epoch: 0131, Loss: 0.5603\n",
      "Epoch: 0132, Loss: 0.5595\n",
      "Epoch: 0133, Loss: 0.5623\n",
      "Epoch: 0134, Loss: 0.5640\n",
      "Epoch: 0135, Loss: 0.5618\n",
      "Epoch: 0136, Loss: 0.5551\n",
      "Epoch: 0137, Loss: 0.5626\n",
      "Epoch: 0138, Loss: 0.5619\n",
      "Epoch: 0139, Loss: 0.5617\n",
      "Epoch: 0140, Loss: 0.5611\n",
      "Epoch: 0141, Loss: 0.5525\n",
      "Epoch: 0142, Loss: 0.5636\n",
      "Epoch: 0143, Loss: 0.5558\n",
      "Epoch: 0144, Loss: 0.5543\n",
      "Epoch: 0145, Loss: 0.5588\n",
      "Epoch: 0146, Loss: 0.5619\n",
      "Epoch: 0147, Loss: 0.5455\n",
      "Epoch: 0148, Loss: 0.5613\n",
      "Epoch: 0149, Loss: 0.5557\n",
      "Epoch: 0150, Loss: 0.5503\n",
      "Epoch: 0151, Loss: 0.5609\n",
      "Epoch: 0152, Loss: 0.5518\n",
      "Epoch: 0153, Loss: 0.5529\n",
      "Epoch: 0154, Loss: 0.5532\n",
      "Epoch: 0155, Loss: 0.5568\n",
      "Epoch: 0156, Loss: 0.5600\n",
      "Epoch: 0157, Loss: 0.5548\n",
      "Epoch: 0158, Loss: 0.5580\n",
      "Epoch: 0159, Loss: 0.5632\n",
      "Epoch: 0160, Loss: 0.5458\n",
      "Epoch: 0161, Loss: 0.5526\n",
      "Epoch: 0162, Loss: 0.5563\n",
      "Epoch: 0163, Loss: 0.5506\n",
      "Epoch: 0164, Loss: 0.5604\n",
      "Epoch: 0165, Loss: 0.5528\n",
      "Epoch: 0166, Loss: 0.5501\n",
      "Epoch: 0167, Loss: 0.5515\n",
      "Epoch: 0168, Loss: 0.5527\n",
      "Epoch: 0169, Loss: 0.5519\n",
      "Epoch: 0170, Loss: 0.5572\n",
      "Epoch: 0171, Loss: 0.5523\n",
      "Epoch: 0172, Loss: 0.5490\n",
      "Epoch: 0173, Loss: 0.5496\n",
      "Epoch: 0174, Loss: 0.5474\n",
      "Epoch: 0175, Loss: 0.5532\n",
      "Epoch: 0176, Loss: 0.5589\n",
      "Epoch: 0177, Loss: 0.5478\n",
      "Epoch: 0178, Loss: 0.5595\n",
      "Epoch: 0179, Loss: 0.5465\n",
      "Epoch: 0180, Loss: 0.5489\n",
      "Epoch: 0181, Loss: 0.5552\n",
      "Epoch: 0182, Loss: 0.5538\n",
      "Epoch: 0183, Loss: 0.5484\n",
      "Epoch: 0184, Loss: 0.5560\n",
      "Epoch: 0185, Loss: 0.5420\n",
      "Epoch: 0186, Loss: 0.5480\n",
      "Epoch: 0187, Loss: 0.5497\n",
      "Epoch: 0188, Loss: 0.5481\n",
      "Epoch: 0189, Loss: 0.5461\n",
      "Epoch: 0190, Loss: 0.5645\n",
      "Epoch: 0191, Loss: 0.5481\n",
      "Epoch: 0192, Loss: 0.5376\n",
      "Epoch: 0193, Loss: 0.5361\n",
      "Epoch: 0194, Loss: 0.5406\n",
      "Epoch: 0195, Loss: 0.5437\n",
      "Epoch: 0196, Loss: 0.5417\n",
      "Epoch: 0197, Loss: 0.5408\n",
      "Epoch: 0198, Loss: 0.5410\n",
      "Epoch: 0199, Loss: 0.5381\n",
      "Epoch: 0200, Loss: 0.5473\n",
      "Epoch: 0201, Loss: 0.5457\n",
      "Epoch: 0202, Loss: 0.5384\n",
      "Epoch: 0203, Loss: 0.5358\n",
      "Epoch: 0204, Loss: 0.5393\n",
      "Epoch: 0205, Loss: 0.5364\n",
      "Epoch: 0206, Loss: 0.5364\n",
      "Epoch: 0207, Loss: 0.5317\n",
      "Epoch: 0208, Loss: 0.5397\n",
      "Epoch: 0209, Loss: 0.5324\n",
      "Epoch: 0210, Loss: 0.5449\n",
      "Epoch: 0211, Loss: 0.5494\n",
      "Epoch: 0212, Loss: 0.5461\n",
      "Epoch: 0213, Loss: 0.5328\n",
      "Epoch: 0214, Loss: 0.5449\n",
      "Epoch: 0215, Loss: 0.5343\n",
      "Epoch: 0216, Loss: 0.5474\n",
      "Epoch: 0217, Loss: 0.5327\n",
      "Epoch: 0218, Loss: 0.5446\n",
      "Epoch: 0219, Loss: 0.5323\n",
      "Epoch: 0220, Loss: 0.5350\n",
      "Epoch: 0221, Loss: 0.5307\n",
      "Epoch: 0222, Loss: 0.5376\n",
      "Epoch: 0223, Loss: 0.5316\n",
      "Epoch: 0224, Loss: 0.5373\n",
      "Epoch: 0225, Loss: 0.5273\n",
      "Epoch: 0226, Loss: 0.5327\n",
      "Epoch: 0227, Loss: 0.5345\n",
      "Epoch: 0228, Loss: 0.5207\n",
      "Epoch: 0229, Loss: 0.5271\n",
      "Epoch: 0230, Loss: 0.5310\n",
      "Epoch: 0231, Loss: 0.5316\n",
      "Epoch: 0232, Loss: 0.5250\n",
      "Epoch: 0233, Loss: 0.5287\n",
      "Epoch: 0234, Loss: 0.5236\n",
      "Epoch: 0235, Loss: 0.5209\n",
      "Epoch: 0236, Loss: 0.5237\n",
      "Epoch: 0237, Loss: 0.5217\n",
      "Epoch: 0238, Loss: 0.5308\n",
      "Epoch: 0239, Loss: 0.5221\n",
      "Epoch: 0240, Loss: 0.5253\n",
      "Epoch: 0241, Loss: 0.5192\n",
      "Epoch: 0242, Loss: 0.5284\n",
      "Epoch: 0243, Loss: 0.5184\n",
      "Epoch: 0244, Loss: 0.5232\n",
      "Epoch: 0245, Loss: 0.5235\n",
      "Epoch: 0246, Loss: 0.5219\n",
      "Epoch: 0247, Loss: 0.5204\n",
      "Epoch: 0248, Loss: 0.5178\n",
      "Epoch: 0249, Loss: 0.5228\n",
      "Epoch: 0250, Loss: 0.5253\n",
      "Epoch: 0251, Loss: 0.5212\n",
      "Epoch: 0252, Loss: 0.5165\n",
      "Epoch: 0253, Loss: 0.5174\n",
      "Epoch: 0254, Loss: 0.5171\n",
      "Epoch: 0255, Loss: 0.5172\n",
      "Epoch: 0256, Loss: 0.5262\n",
      "Epoch: 0257, Loss: 0.5174\n",
      "Epoch: 0258, Loss: 0.5191\n",
      "Epoch: 0259, Loss: 0.5195\n",
      "Epoch: 0260, Loss: 0.5196\n",
      "Epoch: 0261, Loss: 0.5270\n",
      "Epoch: 0262, Loss: 0.5203\n",
      "Epoch: 0263, Loss: 0.5227\n",
      "Epoch: 0264, Loss: 0.5210\n",
      "Epoch: 0265, Loss: 0.5089\n",
      "Epoch: 0266, Loss: 0.5207\n",
      "Epoch: 0267, Loss: 0.5165\n",
      "Epoch: 0268, Loss: 0.5248\n",
      "Epoch: 0269, Loss: 0.5033\n",
      "Epoch: 0270, Loss: 0.5191\n",
      "Epoch: 0271, Loss: 0.5125\n",
      "Epoch: 0272, Loss: 0.5170\n",
      "Epoch: 0273, Loss: 0.5131\n",
      "Epoch: 0274, Loss: 0.5108\n",
      "Epoch: 0275, Loss: 0.5114\n",
      "Epoch: 0276, Loss: 0.5183\n",
      "Epoch: 0277, Loss: 0.5084\n",
      "Epoch: 0278, Loss: 0.5104\n",
      "Epoch: 0279, Loss: 0.5021\n",
      "Epoch: 0280, Loss: 0.5111\n",
      "Epoch: 0281, Loss: 0.5104\n",
      "Epoch: 0282, Loss: 0.5094\n",
      "Epoch: 0283, Loss: 0.5112\n",
      "Epoch: 0284, Loss: 0.5198\n",
      "Epoch: 0285, Loss: 0.5173\n",
      "Epoch: 0286, Loss: 0.5142\n",
      "Epoch: 0287, Loss: 0.5137\n",
      "Epoch: 0288, Loss: 0.5143\n",
      "Epoch: 0289, Loss: 0.5163\n",
      "Epoch: 0290, Loss: 0.5050\n",
      "Epoch: 0291, Loss: 0.5115\n",
      "Epoch: 0292, Loss: 0.5198\n",
      "Epoch: 0293, Loss: 0.5147\n",
      "Epoch: 0294, Loss: 0.5106\n",
      "Epoch: 0295, Loss: 0.5060\n",
      "Epoch: 0296, Loss: 0.5054\n",
      "Epoch: 0297, Loss: 0.5093\n",
      "Epoch: 0298, Loss: 0.5048\n",
      "Epoch: 0299, Loss: 0.5067\n",
      "Epoch: 0300, Loss: 0.5146\n",
      "Epoch: 0301, Loss: 0.5103\n",
      "Epoch: 0302, Loss: 0.5101\n",
      "Epoch: 0303, Loss: 0.5040\n",
      "Epoch: 0304, Loss: 0.5095\n",
      "Epoch: 0305, Loss: 0.5080\n",
      "Epoch: 0306, Loss: 0.5071\n",
      "Epoch: 0307, Loss: 0.5088\n",
      "Epoch: 0308, Loss: 0.5052\n",
      "Epoch: 0309, Loss: 0.5127\n",
      "Epoch: 0310, Loss: 0.5101\n",
      "Epoch: 0311, Loss: 0.5077\n",
      "Epoch: 0312, Loss: 0.5011\n",
      "Epoch: 0313, Loss: 0.5055\n",
      "Epoch: 0314, Loss: 0.4990\n",
      "Epoch: 0315, Loss: 0.5068\n",
      "Epoch: 0316, Loss: 0.5094\n",
      "Epoch: 0317, Loss: 0.4979\n",
      "Epoch: 0318, Loss: 0.5021\n",
      "Epoch: 0319, Loss: 0.5017\n",
      "Epoch: 0320, Loss: 0.4942\n",
      "Epoch: 0321, Loss: 0.5001\n",
      "Epoch: 0322, Loss: 0.5015\n",
      "Epoch: 0323, Loss: 0.4939\n",
      "Epoch: 0324, Loss: 0.4947\n",
      "Epoch: 0325, Loss: 0.4984\n",
      "Epoch: 0326, Loss: 0.4882\n",
      "Epoch: 0327, Loss: 0.4981\n",
      "Epoch: 0328, Loss: 0.4954\n",
      "Epoch: 0329, Loss: 0.4942\n",
      "Epoch: 0330, Loss: 0.5033\n",
      "Epoch: 0331, Loss: 0.4934\n",
      "Epoch: 0332, Loss: 0.5048\n",
      "Epoch: 0333, Loss: 0.4964\n",
      "Epoch: 0334, Loss: 0.4990\n",
      "Epoch: 0335, Loss: 0.4894\n",
      "Epoch: 0336, Loss: 0.4955\n",
      "Epoch: 0337, Loss: 0.4932\n",
      "Epoch: 0338, Loss: 0.4984\n",
      "Epoch: 0339, Loss: 0.4990\n",
      "Epoch: 0340, Loss: 0.4963\n",
      "Epoch: 0341, Loss: 0.4977\n",
      "Epoch: 0342, Loss: 0.4982\n",
      "Epoch: 0343, Loss: 0.4885\n",
      "Epoch: 0344, Loss: 0.4921\n",
      "Epoch: 0345, Loss: 0.4889\n",
      "Epoch: 0346, Loss: 0.4868\n",
      "Epoch: 0347, Loss: 0.4962\n",
      "Epoch: 0348, Loss: 0.4875\n",
      "Epoch: 0349, Loss: 0.4873\n",
      "Epoch: 0350, Loss: 0.4881\n",
      "Epoch: 0351, Loss: 0.4876\n",
      "Epoch: 0352, Loss: 0.4924\n",
      "Epoch: 0353, Loss: 0.4798\n",
      "Epoch: 0354, Loss: 0.4863\n",
      "Epoch: 0355, Loss: 0.4892\n",
      "Epoch: 0356, Loss: 0.4879\n",
      "Epoch: 0357, Loss: 0.4936\n",
      "Epoch: 0358, Loss: 0.4891\n",
      "Epoch: 0359, Loss: 0.4849\n",
      "Epoch: 0360, Loss: 0.4893\n",
      "Epoch: 0361, Loss: 0.4871\n",
      "Epoch: 0362, Loss: 0.4911\n",
      "Epoch: 0363, Loss: 0.4897\n",
      "Epoch: 0364, Loss: 0.4833\n",
      "Epoch: 0365, Loss: 0.4880\n",
      "Epoch: 0366, Loss: 0.4861\n",
      "Epoch: 0367, Loss: 0.4882\n",
      "Epoch: 0368, Loss: 0.4865\n",
      "Epoch: 0369, Loss: 0.4793\n",
      "Epoch: 0370, Loss: 0.4809\n",
      "Epoch: 0371, Loss: 0.4880\n",
      "Epoch: 0372, Loss: 0.4786\n",
      "Epoch: 0373, Loss: 0.4864\n",
      "Epoch: 0374, Loss: 0.4906\n",
      "Epoch: 0375, Loss: 0.4868\n",
      "Epoch: 0376, Loss: 0.4822\n",
      "Epoch: 0377, Loss: 0.4835\n",
      "Epoch: 0378, Loss: 0.4830\n",
      "Epoch: 0379, Loss: 0.4834\n",
      "Epoch: 0380, Loss: 0.4836\n",
      "Epoch: 0381, Loss: 0.4855\n",
      "Epoch: 0382, Loss: 0.4807\n",
      "Epoch: 0383, Loss: 0.4779\n",
      "Epoch: 0384, Loss: 0.4847\n",
      "Epoch: 0385, Loss: 0.4901\n",
      "Epoch: 0386, Loss: 0.4786\n",
      "Epoch: 0387, Loss: 0.4807\n",
      "Epoch: 0388, Loss: 0.4964\n",
      "Epoch: 0389, Loss: 0.4819\n",
      "Epoch: 0390, Loss: 0.4865\n",
      "Epoch: 0391, Loss: 0.4820\n",
      "Epoch: 0392, Loss: 0.4874\n",
      "Epoch: 0393, Loss: 0.4861\n",
      "Epoch: 0394, Loss: 0.4761\n",
      "Epoch: 0395, Loss: 0.4736\n",
      "Epoch: 0396, Loss: 0.4788\n",
      "Epoch: 0397, Loss: 0.4873\n",
      "Epoch: 0398, Loss: 0.4839\n",
      "Epoch: 0399, Loss: 0.4832\n",
      "Epoch: 0400, Loss: 0.4843\n",
      "Epoch: 0401, Loss: 0.4845\n",
      "Epoch: 0402, Loss: 0.4834\n",
      "Epoch: 0403, Loss: 0.4796\n",
      "Epoch: 0404, Loss: 0.4782\n",
      "Epoch: 0405, Loss: 0.4870\n",
      "Epoch: 0406, Loss: 0.4767\n",
      "Epoch: 0407, Loss: 0.4787\n",
      "Epoch: 0408, Loss: 0.4867\n",
      "Epoch: 0409, Loss: 0.4780\n",
      "Epoch: 0410, Loss: 0.4842\n",
      "Epoch: 0411, Loss: 0.4829\n",
      "Epoch: 0412, Loss: 0.4774\n",
      "Epoch: 0413, Loss: 0.4794\n",
      "Epoch: 0414, Loss: 0.4849\n",
      "Epoch: 0415, Loss: 0.4792\n",
      "Epoch: 0416, Loss: 0.4869\n",
      "Epoch: 0417, Loss: 0.4738\n",
      "Epoch: 0418, Loss: 0.4741\n",
      "Epoch: 0419, Loss: 0.4774\n",
      "Epoch: 0420, Loss: 0.4777\n",
      "Epoch: 0421, Loss: 0.4660\n",
      "Epoch: 0422, Loss: 0.4754\n",
      "Epoch: 0423, Loss: 0.4844\n",
      "Epoch: 0424, Loss: 0.4732\n",
      "Epoch: 0425, Loss: 0.4759\n",
      "Epoch: 0426, Loss: 0.4808\n",
      "Epoch: 0427, Loss: 0.4730\n",
      "Epoch: 0428, Loss: 0.4749\n",
      "Epoch: 0429, Loss: 0.4705\n",
      "Epoch: 0430, Loss: 0.4812\n",
      "Epoch: 0431, Loss: 0.4858\n",
      "Epoch: 0432, Loss: 0.4881\n",
      "Epoch: 0433, Loss: 0.4745\n",
      "Epoch: 0434, Loss: 0.4809\n",
      "Epoch: 0435, Loss: 0.4755\n",
      "Epoch: 0436, Loss: 0.4698\n",
      "Epoch: 0437, Loss: 0.4776\n",
      "Epoch: 0438, Loss: 0.4735\n",
      "Epoch: 0439, Loss: 0.4748\n",
      "Epoch: 0440, Loss: 0.4802\n",
      "Epoch: 0441, Loss: 0.4810\n",
      "Epoch: 0442, Loss: 0.4802\n",
      "Epoch: 0443, Loss: 0.4773\n",
      "Epoch: 0444, Loss: 0.4764\n",
      "Epoch: 0445, Loss: 0.4744\n",
      "Epoch: 0446, Loss: 0.4734\n",
      "Epoch: 0447, Loss: 0.4728\n",
      "Epoch: 0448, Loss: 0.4730\n",
      "Epoch: 0449, Loss: 0.4745\n",
      "Epoch: 0450, Loss: 0.4863\n",
      "Epoch: 0451, Loss: 0.4715\n",
      "Epoch: 0452, Loss: 0.4672\n",
      "Epoch: 0453, Loss: 0.4753\n",
      "Epoch: 0454, Loss: 0.4778\n",
      "Epoch: 0455, Loss: 0.4706\n",
      "Epoch: 0456, Loss: 0.4734\n",
      "Epoch: 0457, Loss: 0.4717\n",
      "Epoch: 0458, Loss: 0.4674\n",
      "Epoch: 0459, Loss: 0.4746\n",
      "Epoch: 0460, Loss: 0.4737\n",
      "Epoch: 0461, Loss: 0.4737\n",
      "Epoch: 0462, Loss: 0.4724\n",
      "Epoch: 0463, Loss: 0.4834\n",
      "Epoch: 0464, Loss: 0.4753\n",
      "Epoch: 0465, Loss: 0.4799\n",
      "Epoch: 0466, Loss: 0.4731\n",
      "Epoch: 0467, Loss: 0.4762\n",
      "Epoch: 0468, Loss: 0.4783\n",
      "Epoch: 0469, Loss: 0.4755\n",
      "Epoch: 0470, Loss: 0.4762\n",
      "Epoch: 0471, Loss: 0.4733\n",
      "Epoch: 0472, Loss: 0.4790\n",
      "Epoch: 0473, Loss: 0.4739\n",
      "Epoch: 0474, Loss: 0.4679\n",
      "Epoch: 0475, Loss: 0.4732\n",
      "Epoch: 0476, Loss: 0.4696\n",
      "Epoch: 0477, Loss: 0.4777\n",
      "Epoch: 0478, Loss: 0.4777\n",
      "Epoch: 0479, Loss: 0.4651\n",
      "Epoch: 0480, Loss: 0.4697\n",
      "Epoch: 0481, Loss: 0.4685\n",
      "Epoch: 0482, Loss: 0.4705\n",
      "Epoch: 0483, Loss: 0.4654\n",
      "Epoch: 0484, Loss: 0.4711\n",
      "Epoch: 0485, Loss: 0.4703\n",
      "Epoch: 0486, Loss: 0.4662\n",
      "Epoch: 0487, Loss: 0.4676\n",
      "Epoch: 0488, Loss: 0.4662\n",
      "Epoch: 0489, Loss: 0.4672\n",
      "Epoch: 0490, Loss: 0.4689\n",
      "Epoch: 0491, Loss: 0.4651\n",
      "Epoch: 0492, Loss: 0.4667\n",
      "Epoch: 0493, Loss: 0.4742\n",
      "Epoch: 0494, Loss: 0.4661\n",
      "Epoch: 0495, Loss: 0.4693\n",
      "Epoch: 0496, Loss: 0.4622\n",
      "Epoch: 0497, Loss: 0.4718\n",
      "Epoch: 0498, Loss: 0.4697\n",
      "Epoch: 0499, Loss: 0.4691\n",
      "Epoch: 0001, Loss: 0.6975\n",
      "Epoch: 0002, Loss: 0.6934\n",
      "Epoch: 0003, Loss: 0.7168\n",
      "Epoch: 0004, Loss: 0.6932\n",
      "Epoch: 0005, Loss: 0.6936\n",
      "Epoch: 0006, Loss: 0.6941\n",
      "Epoch: 0007, Loss: 0.6941\n",
      "Epoch: 0008, Loss: 0.6937\n",
      "Epoch: 0009, Loss: 0.6922\n",
      "Epoch: 0010, Loss: 0.6891\n",
      "Epoch: 0011, Loss: 0.6827\n",
      "Epoch: 0012, Loss: 0.6703\n",
      "Epoch: 0013, Loss: 0.6561\n",
      "Epoch: 0014, Loss: 0.6442\n",
      "Epoch: 0015, Loss: 0.6383\n",
      "Epoch: 0016, Loss: 0.6421\n",
      "Epoch: 0017, Loss: 0.6406\n",
      "Epoch: 0018, Loss: 0.6318\n",
      "Epoch: 0019, Loss: 0.6184\n",
      "Epoch: 0020, Loss: 0.6156\n",
      "Epoch: 0021, Loss: 0.6062\n",
      "Epoch: 0022, Loss: 0.6080\n",
      "Epoch: 0023, Loss: 0.6064\n",
      "Epoch: 0024, Loss: 0.6032\n",
      "Epoch: 0025, Loss: 0.5942\n",
      "Epoch: 0026, Loss: 0.6018\n",
      "Epoch: 0027, Loss: 0.5979\n",
      "Epoch: 0028, Loss: 0.5954\n",
      "Epoch: 0029, Loss: 0.6024\n",
      "Epoch: 0030, Loss: 0.5863\n",
      "Epoch: 0031, Loss: 0.5912\n",
      "Epoch: 0032, Loss: 0.5917\n",
      "Epoch: 0033, Loss: 0.5869\n",
      "Epoch: 0034, Loss: 0.5906\n",
      "Epoch: 0035, Loss: 0.5812\n",
      "Epoch: 0036, Loss: 0.5831\n",
      "Epoch: 0037, Loss: 0.5856\n",
      "Epoch: 0038, Loss: 0.5834\n",
      "Epoch: 0039, Loss: 0.5774\n",
      "Epoch: 0040, Loss: 0.5667\n",
      "Epoch: 0041, Loss: 0.5777\n",
      "Epoch: 0042, Loss: 0.5777\n",
      "Epoch: 0043, Loss: 0.5764\n",
      "Epoch: 0044, Loss: 0.5764\n",
      "Epoch: 0045, Loss: 0.5751\n",
      "Epoch: 0046, Loss: 0.5716\n",
      "Epoch: 0047, Loss: 0.5744\n",
      "Epoch: 0048, Loss: 0.5683\n",
      "Epoch: 0049, Loss: 0.5762\n",
      "Epoch: 0050, Loss: 0.5662\n",
      "Epoch: 0051, Loss: 0.5711\n",
      "Epoch: 0052, Loss: 0.5690\n",
      "Epoch: 0053, Loss: 0.5702\n",
      "Epoch: 0054, Loss: 0.5686\n",
      "Epoch: 0055, Loss: 0.5709\n",
      "Epoch: 0056, Loss: 0.5738\n",
      "Epoch: 0057, Loss: 0.5626\n",
      "Epoch: 0058, Loss: 0.5793\n",
      "Epoch: 0059, Loss: 0.5636\n",
      "Epoch: 0060, Loss: 0.5712\n",
      "Epoch: 0061, Loss: 0.5712\n",
      "Epoch: 0062, Loss: 0.5667\n",
      "Epoch: 0063, Loss: 0.5618\n",
      "Epoch: 0064, Loss: 0.5672\n",
      "Epoch: 0065, Loss: 0.5728\n",
      "Epoch: 0066, Loss: 0.5585\n",
      "Epoch: 0067, Loss: 0.5619\n",
      "Epoch: 0068, Loss: 0.5614\n",
      "Epoch: 0069, Loss: 0.5633\n",
      "Epoch: 0070, Loss: 0.5607\n",
      "Epoch: 0071, Loss: 0.5609\n",
      "Epoch: 0072, Loss: 0.5638\n",
      "Epoch: 0073, Loss: 0.5625\n",
      "Epoch: 0074, Loss: 0.5665\n",
      "Epoch: 0075, Loss: 0.5493\n",
      "Epoch: 0076, Loss: 0.5687\n",
      "Epoch: 0077, Loss: 0.5576\n",
      "Epoch: 0078, Loss: 0.5651\n",
      "Epoch: 0079, Loss: 0.5553\n",
      "Epoch: 0080, Loss: 0.5641\n",
      "Epoch: 0081, Loss: 0.5621\n",
      "Epoch: 0082, Loss: 0.5610\n",
      "Epoch: 0083, Loss: 0.5622\n",
      "Epoch: 0084, Loss: 0.5654\n",
      "Epoch: 0085, Loss: 0.5635\n",
      "Epoch: 0086, Loss: 0.5617\n",
      "Epoch: 0087, Loss: 0.5561\n",
      "Epoch: 0088, Loss: 0.5653\n",
      "Epoch: 0089, Loss: 0.5567\n",
      "Epoch: 0090, Loss: 0.5607\n",
      "Epoch: 0091, Loss: 0.5540\n",
      "Epoch: 0092, Loss: 0.5542\n",
      "Epoch: 0093, Loss: 0.5587\n",
      "Epoch: 0094, Loss: 0.5583\n",
      "Epoch: 0095, Loss: 0.5488\n",
      "Epoch: 0096, Loss: 0.5569\n",
      "Epoch: 0097, Loss: 0.5522\n",
      "Epoch: 0098, Loss: 0.5617\n",
      "Epoch: 0099, Loss: 0.5563\n",
      "Epoch: 0100, Loss: 0.5680\n",
      "Epoch: 0101, Loss: 0.5540\n",
      "Epoch: 0102, Loss: 0.5601\n",
      "Epoch: 0103, Loss: 0.5532\n",
      "Epoch: 0104, Loss: 0.5563\n",
      "Epoch: 0105, Loss: 0.5479\n",
      "Epoch: 0106, Loss: 0.5592\n",
      "Epoch: 0107, Loss: 0.5553\n",
      "Epoch: 0108, Loss: 0.5567\n",
      "Epoch: 0109, Loss: 0.5508\n",
      "Epoch: 0110, Loss: 0.5546\n",
      "Epoch: 0111, Loss: 0.5518\n",
      "Epoch: 0112, Loss: 0.5482\n",
      "Epoch: 0113, Loss: 0.5523\n",
      "Epoch: 0114, Loss: 0.5561\n",
      "Epoch: 0115, Loss: 0.5501\n",
      "Epoch: 0116, Loss: 0.5524\n",
      "Epoch: 0117, Loss: 0.5481\n",
      "Epoch: 0118, Loss: 0.5483\n",
      "Epoch: 0119, Loss: 0.5514\n",
      "Epoch: 0120, Loss: 0.5477\n",
      "Epoch: 0121, Loss: 0.5450\n",
      "Epoch: 0122, Loss: 0.5463\n",
      "Epoch: 0123, Loss: 0.5443\n",
      "Epoch: 0124, Loss: 0.5526\n",
      "Epoch: 0125, Loss: 0.5429\n",
      "Epoch: 0126, Loss: 0.5549\n",
      "Epoch: 0127, Loss: 0.5406\n",
      "Epoch: 0128, Loss: 0.5466\n",
      "Epoch: 0129, Loss: 0.5495\n",
      "Epoch: 0130, Loss: 0.5449\n",
      "Epoch: 0131, Loss: 0.5478\n",
      "Epoch: 0132, Loss: 0.5421\n",
      "Epoch: 0133, Loss: 0.5456\n",
      "Epoch: 0134, Loss: 0.5468\n",
      "Epoch: 0135, Loss: 0.5449\n",
      "Epoch: 0136, Loss: 0.5432\n",
      "Epoch: 0137, Loss: 0.5441\n",
      "Epoch: 0138, Loss: 0.5465\n",
      "Epoch: 0139, Loss: 0.5456\n",
      "Epoch: 0140, Loss: 0.5453\n",
      "Epoch: 0141, Loss: 0.5422\n",
      "Epoch: 0142, Loss: 0.5437\n",
      "Epoch: 0143, Loss: 0.5415\n",
      "Epoch: 0144, Loss: 0.5507\n",
      "Epoch: 0145, Loss: 0.5427\n",
      "Epoch: 0146, Loss: 0.5476\n",
      "Epoch: 0147, Loss: 0.5452\n",
      "Epoch: 0148, Loss: 0.5373\n",
      "Epoch: 0149, Loss: 0.5386\n",
      "Epoch: 0150, Loss: 0.5447\n",
      "Epoch: 0151, Loss: 0.5431\n",
      "Epoch: 0152, Loss: 0.5424\n",
      "Epoch: 0153, Loss: 0.5391\n",
      "Epoch: 0154, Loss: 0.5524\n",
      "Epoch: 0155, Loss: 0.5402\n",
      "Epoch: 0156, Loss: 0.5352\n",
      "Epoch: 0157, Loss: 0.5417\n",
      "Epoch: 0158, Loss: 0.5356\n",
      "Epoch: 0159, Loss: 0.5468\n",
      "Epoch: 0160, Loss: 0.5481\n",
      "Epoch: 0161, Loss: 0.5386\n",
      "Epoch: 0162, Loss: 0.5345\n",
      "Epoch: 0163, Loss: 0.5442\n",
      "Epoch: 0164, Loss: 0.5411\n",
      "Epoch: 0165, Loss: 0.5385\n",
      "Epoch: 0166, Loss: 0.5439\n",
      "Epoch: 0167, Loss: 0.5360\n",
      "Epoch: 0168, Loss: 0.5399\n",
      "Epoch: 0169, Loss: 0.5346\n",
      "Epoch: 0170, Loss: 0.5400\n",
      "Epoch: 0171, Loss: 0.5385\n",
      "Epoch: 0172, Loss: 0.5344\n",
      "Epoch: 0173, Loss: 0.5428\n",
      "Epoch: 0174, Loss: 0.5390\n",
      "Epoch: 0175, Loss: 0.5350\n",
      "Epoch: 0176, Loss: 0.5272\n",
      "Epoch: 0177, Loss: 0.5319\n",
      "Epoch: 0178, Loss: 0.5403\n",
      "Epoch: 0179, Loss: 0.5378\n",
      "Epoch: 0180, Loss: 0.5366\n",
      "Epoch: 0181, Loss: 0.5285\n",
      "Epoch: 0182, Loss: 0.5419\n",
      "Epoch: 0183, Loss: 0.5296\n",
      "Epoch: 0184, Loss: 0.5312\n",
      "Epoch: 0185, Loss: 0.5325\n",
      "Epoch: 0186, Loss: 0.5320\n",
      "Epoch: 0187, Loss: 0.5337\n",
      "Epoch: 0188, Loss: 0.5257\n",
      "Epoch: 0189, Loss: 0.5334\n",
      "Epoch: 0190, Loss: 0.5301\n",
      "Epoch: 0191, Loss: 0.5298\n",
      "Epoch: 0192, Loss: 0.5459\n",
      "Epoch: 0193, Loss: 0.5303\n",
      "Epoch: 0194, Loss: 0.5291\n",
      "Epoch: 0195, Loss: 0.5308\n",
      "Epoch: 0196, Loss: 0.5220\n",
      "Epoch: 0197, Loss: 0.5261\n",
      "Epoch: 0198, Loss: 0.5256\n",
      "Epoch: 0199, Loss: 0.5273\n",
      "Epoch: 0200, Loss: 0.5256\n",
      "Epoch: 0201, Loss: 0.5221\n",
      "Epoch: 0202, Loss: 0.5145\n",
      "Epoch: 0203, Loss: 0.5092\n",
      "Epoch: 0204, Loss: 0.5199\n",
      "Epoch: 0205, Loss: 0.5174\n",
      "Epoch: 0206, Loss: 0.5204\n",
      "Epoch: 0207, Loss: 0.5202\n",
      "Epoch: 0208, Loss: 0.5212\n",
      "Epoch: 0209, Loss: 0.5067\n",
      "Epoch: 0210, Loss: 0.5252\n",
      "Epoch: 0211, Loss: 0.5153\n",
      "Epoch: 0212, Loss: 0.5103\n",
      "Epoch: 0213, Loss: 0.5247\n",
      "Epoch: 0214, Loss: 0.5221\n",
      "Epoch: 0215, Loss: 0.5121\n",
      "Epoch: 0216, Loss: 0.5130\n",
      "Epoch: 0217, Loss: 0.5117\n",
      "Epoch: 0218, Loss: 0.5111\n",
      "Epoch: 0219, Loss: 0.4980\n",
      "Epoch: 0220, Loss: 0.5133\n",
      "Epoch: 0221, Loss: 0.5136\n",
      "Epoch: 0222, Loss: 0.5140\n",
      "Epoch: 0223, Loss: 0.5094\n",
      "Epoch: 0224, Loss: 0.5101\n",
      "Epoch: 0225, Loss: 0.5130\n",
      "Epoch: 0226, Loss: 0.5124\n",
      "Epoch: 0227, Loss: 0.5118\n",
      "Epoch: 0228, Loss: 0.5158\n",
      "Epoch: 0229, Loss: 0.5175\n",
      "Epoch: 0230, Loss: 0.5088\n",
      "Epoch: 0231, Loss: 0.5141\n",
      "Epoch: 0232, Loss: 0.5069\n",
      "Epoch: 0233, Loss: 0.5076\n",
      "Epoch: 0234, Loss: 0.5176\n",
      "Epoch: 0235, Loss: 0.5092\n",
      "Epoch: 0236, Loss: 0.5053\n",
      "Epoch: 0237, Loss: 0.5069\n",
      "Epoch: 0238, Loss: 0.5109\n",
      "Epoch: 0239, Loss: 0.5064\n",
      "Epoch: 0240, Loss: 0.5038\n",
      "Epoch: 0241, Loss: 0.5102\n",
      "Epoch: 0242, Loss: 0.5122\n",
      "Epoch: 0243, Loss: 0.5035\n",
      "Epoch: 0244, Loss: 0.5047\n",
      "Epoch: 0245, Loss: 0.5058\n",
      "Epoch: 0246, Loss: 0.5178\n",
      "Epoch: 0247, Loss: 0.5049\n",
      "Epoch: 0248, Loss: 0.5087\n",
      "Epoch: 0249, Loss: 0.5099\n",
      "Epoch: 0250, Loss: 0.5027\n",
      "Epoch: 0251, Loss: 0.5015\n",
      "Epoch: 0252, Loss: 0.5117\n",
      "Epoch: 0253, Loss: 0.5119\n",
      "Epoch: 0254, Loss: 0.5036\n",
      "Epoch: 0255, Loss: 0.5141\n",
      "Epoch: 0256, Loss: 0.5066\n",
      "Epoch: 0257, Loss: 0.5102\n",
      "Epoch: 0258, Loss: 0.5018\n",
      "Epoch: 0259, Loss: 0.5069\n",
      "Epoch: 0260, Loss: 0.5056\n",
      "Epoch: 0261, Loss: 0.5052\n",
      "Epoch: 0262, Loss: 0.5041\n",
      "Epoch: 0263, Loss: 0.5081\n",
      "Epoch: 0264, Loss: 0.5039\n",
      "Epoch: 0265, Loss: 0.5029\n",
      "Epoch: 0266, Loss: 0.5054\n",
      "Epoch: 0267, Loss: 0.5033\n",
      "Epoch: 0268, Loss: 0.5032\n",
      "Epoch: 0269, Loss: 0.5042\n",
      "Epoch: 0270, Loss: 0.5192\n",
      "Epoch: 0271, Loss: 0.5081\n",
      "Epoch: 0272, Loss: 0.5189\n",
      "Epoch: 0273, Loss: 0.4951\n",
      "Epoch: 0274, Loss: 0.5028\n",
      "Epoch: 0275, Loss: 0.4967\n",
      "Epoch: 0276, Loss: 0.5004\n",
      "Epoch: 0277, Loss: 0.5080\n",
      "Epoch: 0278, Loss: 0.4968\n",
      "Epoch: 0279, Loss: 0.5125\n",
      "Epoch: 0280, Loss: 0.4977\n",
      "Epoch: 0281, Loss: 0.5065\n",
      "Epoch: 0282, Loss: 0.4965\n",
      "Epoch: 0283, Loss: 0.5055\n",
      "Epoch: 0284, Loss: 0.5037\n",
      "Epoch: 0285, Loss: 0.5101\n",
      "Epoch: 0286, Loss: 0.4994\n",
      "Epoch: 0287, Loss: 0.5187\n",
      "Epoch: 0288, Loss: 0.5033\n",
      "Epoch: 0289, Loss: 0.5037\n",
      "Epoch: 0290, Loss: 0.4978\n",
      "Epoch: 0291, Loss: 0.4987\n",
      "Epoch: 0292, Loss: 0.5015\n",
      "Epoch: 0293, Loss: 0.5026\n",
      "Epoch: 0294, Loss: 0.4962\n",
      "Epoch: 0295, Loss: 0.5004\n",
      "Epoch: 0296, Loss: 0.4977\n",
      "Epoch: 0297, Loss: 0.5012\n",
      "Epoch: 0298, Loss: 0.5024\n",
      "Epoch: 0299, Loss: 0.5073\n",
      "Epoch: 0300, Loss: 0.5010\n",
      "Epoch: 0301, Loss: 0.5040\n",
      "Epoch: 0302, Loss: 0.5019\n",
      "Epoch: 0303, Loss: 0.5114\n",
      "Epoch: 0304, Loss: 0.5014\n",
      "Epoch: 0305, Loss: 0.5038\n",
      "Epoch: 0306, Loss: 0.5057\n",
      "Epoch: 0307, Loss: 0.5093\n",
      "Epoch: 0308, Loss: 0.4956\n",
      "Epoch: 0309, Loss: 0.4977\n",
      "Epoch: 0310, Loss: 0.4978\n",
      "Epoch: 0311, Loss: 0.5050\n",
      "Epoch: 0312, Loss: 0.4995\n",
      "Epoch: 0313, Loss: 0.5039\n",
      "Epoch: 0314, Loss: 0.4928\n",
      "Epoch: 0315, Loss: 0.5008\n",
      "Epoch: 0316, Loss: 0.4931\n",
      "Epoch: 0317, Loss: 0.5005\n",
      "Epoch: 0318, Loss: 0.5048\n",
      "Epoch: 0319, Loss: 0.4943\n",
      "Epoch: 0320, Loss: 0.4996\n",
      "Epoch: 0321, Loss: 0.4965\n",
      "Epoch: 0322, Loss: 0.4995\n",
      "Epoch: 0323, Loss: 0.4991\n",
      "Epoch: 0324, Loss: 0.5032\n",
      "Epoch: 0325, Loss: 0.5054\n",
      "Epoch: 0326, Loss: 0.4963\n",
      "Epoch: 0327, Loss: 0.4955\n",
      "Epoch: 0328, Loss: 0.4930\n",
      "Epoch: 0329, Loss: 0.4988\n",
      "Epoch: 0330, Loss: 0.5027\n",
      "Epoch: 0331, Loss: 0.4999\n",
      "Epoch: 0332, Loss: 0.4957\n",
      "Epoch: 0333, Loss: 0.4961\n",
      "Epoch: 0334, Loss: 0.4984\n",
      "Epoch: 0335, Loss: 0.4998\n",
      "Epoch: 0336, Loss: 0.4953\n",
      "Epoch: 0337, Loss: 0.5000\n",
      "Epoch: 0338, Loss: 0.4920\n",
      "Epoch: 0339, Loss: 0.4963\n",
      "Epoch: 0340, Loss: 0.4895\n",
      "Epoch: 0341, Loss: 0.4915\n",
      "Epoch: 0342, Loss: 0.4995\n",
      "Epoch: 0343, Loss: 0.4917\n",
      "Epoch: 0344, Loss: 0.5018\n",
      "Epoch: 0345, Loss: 0.4909\n",
      "Epoch: 0346, Loss: 0.4936\n",
      "Epoch: 0347, Loss: 0.4962\n",
      "Epoch: 0348, Loss: 0.4980\n",
      "Epoch: 0349, Loss: 0.4951\n",
      "Epoch: 0350, Loss: 0.4867\n",
      "Epoch: 0351, Loss: 0.5004\n",
      "Epoch: 0352, Loss: 0.4921\n",
      "Epoch: 0353, Loss: 0.4923\n",
      "Epoch: 0354, Loss: 0.5002\n",
      "Epoch: 0355, Loss: 0.4898\n",
      "Epoch: 0356, Loss: 0.4961\n",
      "Epoch: 0357, Loss: 0.4919\n",
      "Epoch: 0358, Loss: 0.4875\n",
      "Epoch: 0359, Loss: 0.4869\n",
      "Epoch: 0360, Loss: 0.4876\n",
      "Epoch: 0361, Loss: 0.4993\n",
      "Epoch: 0362, Loss: 0.4898\n",
      "Epoch: 0363, Loss: 0.4964\n",
      "Epoch: 0364, Loss: 0.4887\n",
      "Epoch: 0365, Loss: 0.4921\n",
      "Epoch: 0366, Loss: 0.4948\n",
      "Epoch: 0367, Loss: 0.4853\n",
      "Epoch: 0368, Loss: 0.4873\n",
      "Epoch: 0369, Loss: 0.4850\n",
      "Epoch: 0370, Loss: 0.4835\n",
      "Epoch: 0371, Loss: 0.4791\n",
      "Epoch: 0372, Loss: 0.4925\n",
      "Epoch: 0373, Loss: 0.4801\n",
      "Epoch: 0374, Loss: 0.4868\n",
      "Epoch: 0375, Loss: 0.4757\n",
      "Epoch: 0376, Loss: 0.4878\n",
      "Epoch: 0377, Loss: 0.4848\n",
      "Epoch: 0378, Loss: 0.4853\n",
      "Epoch: 0379, Loss: 0.4739\n",
      "Epoch: 0380, Loss: 0.4898\n",
      "Epoch: 0381, Loss: 0.4870\n",
      "Epoch: 0382, Loss: 0.4926\n",
      "Epoch: 0383, Loss: 0.4894\n",
      "Epoch: 0384, Loss: 0.4819\n",
      "Epoch: 0385, Loss: 0.4904\n",
      "Epoch: 0386, Loss: 0.4910\n",
      "Epoch: 0387, Loss: 0.4835\n",
      "Epoch: 0388, Loss: 0.4858\n",
      "Epoch: 0389, Loss: 0.4835\n",
      "Epoch: 0390, Loss: 0.4896\n",
      "Epoch: 0391, Loss: 0.4813\n",
      "Epoch: 0392, Loss: 0.4835\n",
      "Epoch: 0393, Loss: 0.4666\n",
      "Epoch: 0394, Loss: 0.4807\n",
      "Epoch: 0395, Loss: 0.4782\n",
      "Epoch: 0396, Loss: 0.4768\n",
      "Epoch: 0397, Loss: 0.4870\n",
      "Epoch: 0398, Loss: 0.4715\n",
      "Epoch: 0399, Loss: 0.4861\n",
      "Epoch: 0400, Loss: 0.4841\n",
      "Epoch: 0401, Loss: 0.4889\n",
      "Epoch: 0402, Loss: 0.4756\n",
      "Epoch: 0403, Loss: 0.4841\n",
      "Epoch: 0404, Loss: 0.4758\n",
      "Epoch: 0405, Loss: 0.4736\n",
      "Epoch: 0406, Loss: 0.4738\n",
      "Epoch: 0407, Loss: 0.4946\n",
      "Epoch: 0408, Loss: 0.4699\n",
      "Epoch: 0409, Loss: 0.4824\n",
      "Epoch: 0410, Loss: 0.4760\n",
      "Epoch: 0411, Loss: 0.4791\n",
      "Epoch: 0412, Loss: 0.4867\n",
      "Epoch: 0413, Loss: 0.4849\n",
      "Epoch: 0414, Loss: 0.4768\n",
      "Epoch: 0415, Loss: 0.4841\n",
      "Epoch: 0416, Loss: 0.4837\n",
      "Epoch: 0417, Loss: 0.4825\n",
      "Epoch: 0418, Loss: 0.4709\n",
      "Epoch: 0419, Loss: 0.4786\n",
      "Epoch: 0420, Loss: 0.4783\n",
      "Epoch: 0421, Loss: 0.4845\n",
      "Epoch: 0422, Loss: 0.4833\n",
      "Epoch: 0423, Loss: 0.4744\n",
      "Epoch: 0424, Loss: 0.4788\n",
      "Epoch: 0425, Loss: 0.4729\n",
      "Epoch: 0426, Loss: 0.4801\n",
      "Epoch: 0427, Loss: 0.4778\n",
      "Epoch: 0428, Loss: 0.4893\n",
      "Epoch: 0429, Loss: 0.4840\n",
      "Epoch: 0430, Loss: 0.4795\n",
      "Epoch: 0431, Loss: 0.4796\n",
      "Epoch: 0432, Loss: 0.4777\n",
      "Epoch: 0433, Loss: 0.4811\n",
      "Epoch: 0434, Loss: 0.4807\n",
      "Epoch: 0435, Loss: 0.4813\n",
      "Epoch: 0436, Loss: 0.4773\n",
      "Epoch: 0437, Loss: 0.4803\n",
      "Epoch: 0438, Loss: 0.4745\n",
      "Epoch: 0439, Loss: 0.4740\n",
      "Epoch: 0440, Loss: 0.4782\n",
      "Epoch: 0441, Loss: 0.4788\n",
      "Epoch: 0442, Loss: 0.4725\n",
      "Epoch: 0443, Loss: 0.4764\n",
      "Epoch: 0444, Loss: 0.4767\n",
      "Epoch: 0445, Loss: 0.4645\n",
      "Epoch: 0446, Loss: 0.4784\n",
      "Epoch: 0447, Loss: 0.4872\n",
      "Epoch: 0448, Loss: 0.4793\n",
      "Epoch: 0449, Loss: 0.4801\n",
      "Epoch: 0450, Loss: 0.4816\n",
      "Epoch: 0451, Loss: 0.4667\n",
      "Epoch: 0452, Loss: 0.4782\n",
      "Epoch: 0453, Loss: 0.4789\n",
      "Epoch: 0454, Loss: 0.4592\n",
      "Epoch: 0455, Loss: 0.4859\n",
      "Epoch: 0456, Loss: 0.4754\n",
      "Epoch: 0457, Loss: 0.4795\n",
      "Epoch: 0458, Loss: 0.4738\n",
      "Epoch: 0459, Loss: 0.4825\n",
      "Epoch: 0460, Loss: 0.4859\n",
      "Epoch: 0461, Loss: 0.4733\n",
      "Epoch: 0462, Loss: 0.4709\n",
      "Epoch: 0463, Loss: 0.4790\n",
      "Epoch: 0464, Loss: 0.4782\n",
      "Epoch: 0465, Loss: 0.4802\n",
      "Epoch: 0466, Loss: 0.4879\n",
      "Epoch: 0467, Loss: 0.4701\n",
      "Epoch: 0468, Loss: 0.4806\n",
      "Epoch: 0469, Loss: 0.4727\n",
      "Epoch: 0470, Loss: 0.4762\n",
      "Epoch: 0471, Loss: 0.4761\n",
      "Epoch: 0472, Loss: 0.4792\n",
      "Epoch: 0473, Loss: 0.4702\n",
      "Epoch: 0474, Loss: 0.4728\n",
      "Epoch: 0475, Loss: 0.4757\n",
      "Epoch: 0476, Loss: 0.4746\n",
      "Epoch: 0477, Loss: 0.4854\n",
      "Epoch: 0478, Loss: 0.4807\n",
      "Epoch: 0479, Loss: 0.4863\n",
      "Epoch: 0480, Loss: 0.4742\n",
      "Epoch: 0481, Loss: 0.4727\n",
      "Epoch: 0482, Loss: 0.4723\n",
      "Epoch: 0483, Loss: 0.4810\n",
      "Epoch: 0484, Loss: 0.4825\n",
      "Epoch: 0485, Loss: 0.4792\n",
      "Epoch: 0486, Loss: 0.4805\n",
      "Epoch: 0487, Loss: 0.4650\n",
      "Epoch: 0488, Loss: 0.4833\n",
      "Epoch: 0489, Loss: 0.4627\n",
      "Epoch: 0490, Loss: 0.4639\n",
      "Epoch: 0491, Loss: 0.4776\n",
      "Epoch: 0492, Loss: 0.4686\n",
      "Epoch: 0493, Loss: 0.4703\n",
      "Epoch: 0494, Loss: 0.4681\n",
      "Epoch: 0495, Loss: 0.4790\n",
      "Epoch: 0496, Loss: 0.4725\n",
      "Epoch: 0497, Loss: 0.4690\n",
      "Epoch: 0498, Loss: 0.4781\n",
      "Epoch: 0499, Loss: 0.4743\n",
      "Epoch: 0001, Loss: 0.6965\n",
      "Epoch: 0002, Loss: 0.6934\n",
      "Epoch: 0003, Loss: 0.7113\n",
      "Epoch: 0004, Loss: 0.6931\n",
      "Epoch: 0005, Loss: 0.6934\n",
      "Epoch: 0006, Loss: 0.6936\n",
      "Epoch: 0007, Loss: 0.6936\n",
      "Epoch: 0008, Loss: 0.6935\n",
      "Epoch: 0009, Loss: 0.6933\n",
      "Epoch: 0010, Loss: 0.6929\n",
      "Epoch: 0011, Loss: 0.6923\n",
      "Epoch: 0012, Loss: 0.6914\n",
      "Epoch: 0013, Loss: 0.6903\n",
      "Epoch: 0014, Loss: 0.6885\n",
      "Epoch: 0015, Loss: 0.6859\n",
      "Epoch: 0016, Loss: 0.6852\n",
      "Epoch: 0017, Loss: 0.6825\n",
      "Epoch: 0018, Loss: 0.6767\n",
      "Epoch: 0019, Loss: 0.6687\n",
      "Epoch: 0020, Loss: 0.6667\n",
      "Epoch: 0021, Loss: 0.6730\n",
      "Epoch: 0022, Loss: 0.6669\n",
      "Epoch: 0023, Loss: 0.6742\n",
      "Epoch: 0024, Loss: 0.6512\n",
      "Epoch: 0025, Loss: 0.6776\n",
      "Epoch: 0026, Loss: 0.6489\n",
      "Epoch: 0027, Loss: 0.6531\n",
      "Epoch: 0028, Loss: 0.6573\n",
      "Epoch: 0029, Loss: 0.6502\n",
      "Epoch: 0030, Loss: 0.6448\n",
      "Epoch: 0031, Loss: 0.6478\n",
      "Epoch: 0032, Loss: 0.6379\n",
      "Epoch: 0033, Loss: 0.6451\n",
      "Epoch: 0034, Loss: 0.6412\n",
      "Epoch: 0035, Loss: 0.6317\n",
      "Epoch: 0036, Loss: 0.6414\n",
      "Epoch: 0037, Loss: 0.6305\n",
      "Epoch: 0038, Loss: 0.6274\n",
      "Epoch: 0039, Loss: 0.6378\n",
      "Epoch: 0040, Loss: 0.6223\n",
      "Epoch: 0041, Loss: 0.6176\n",
      "Epoch: 0042, Loss: 0.6304\n",
      "Epoch: 0043, Loss: 0.6221\n",
      "Epoch: 0044, Loss: 0.6205\n",
      "Epoch: 0045, Loss: 0.6182\n",
      "Epoch: 0046, Loss: 0.6146\n",
      "Epoch: 0047, Loss: 0.6165\n",
      "Epoch: 0048, Loss: 0.6096\n",
      "Epoch: 0049, Loss: 0.6050\n",
      "Epoch: 0050, Loss: 0.6070\n",
      "Epoch: 0051, Loss: 0.6139\n",
      "Epoch: 0052, Loss: 0.5998\n",
      "Epoch: 0053, Loss: 0.5988\n",
      "Epoch: 0054, Loss: 0.6065\n",
      "Epoch: 0055, Loss: 0.6044\n",
      "Epoch: 0056, Loss: 0.5956\n",
      "Epoch: 0057, Loss: 0.5938\n",
      "Epoch: 0058, Loss: 0.5909\n",
      "Epoch: 0059, Loss: 0.5982\n",
      "Epoch: 0060, Loss: 0.6101\n",
      "Epoch: 0061, Loss: 0.5869\n",
      "Epoch: 0062, Loss: 0.5905\n",
      "Epoch: 0063, Loss: 0.5921\n",
      "Epoch: 0064, Loss: 0.5952\n",
      "Epoch: 0065, Loss: 0.5944\n",
      "Epoch: 0066, Loss: 0.5881\n",
      "Epoch: 0067, Loss: 0.5922\n",
      "Epoch: 0068, Loss: 0.5900\n",
      "Epoch: 0069, Loss: 0.5790\n",
      "Epoch: 0070, Loss: 0.5851\n",
      "Epoch: 0071, Loss: 0.5970\n",
      "Epoch: 0072, Loss: 0.5940\n",
      "Epoch: 0073, Loss: 0.5872\n",
      "Epoch: 0074, Loss: 0.5879\n",
      "Epoch: 0075, Loss: 0.5893\n",
      "Epoch: 0076, Loss: 0.5785\n",
      "Epoch: 0077, Loss: 0.5857\n",
      "Epoch: 0078, Loss: 0.5889\n",
      "Epoch: 0079, Loss: 0.5772\n",
      "Epoch: 0080, Loss: 0.5929\n",
      "Epoch: 0081, Loss: 0.5929\n",
      "Epoch: 0082, Loss: 0.5816\n",
      "Epoch: 0083, Loss: 0.5897\n",
      "Epoch: 0084, Loss: 0.5741\n",
      "Epoch: 0085, Loss: 0.5885\n",
      "Epoch: 0086, Loss: 0.5893\n",
      "Epoch: 0087, Loss: 0.5865\n",
      "Epoch: 0088, Loss: 0.5781\n",
      "Epoch: 0089, Loss: 0.5810\n",
      "Epoch: 0090, Loss: 0.5791\n",
      "Epoch: 0091, Loss: 0.5679\n",
      "Epoch: 0092, Loss: 0.5814\n",
      "Epoch: 0093, Loss: 0.5743\n",
      "Epoch: 0094, Loss: 0.5790\n",
      "Epoch: 0095, Loss: 0.5731\n",
      "Epoch: 0096, Loss: 0.5699\n",
      "Epoch: 0097, Loss: 0.5593\n",
      "Epoch: 0098, Loss: 0.5865\n",
      "Epoch: 0099, Loss: 0.5923\n",
      "Epoch: 0100, Loss: 0.5683\n",
      "Epoch: 0101, Loss: 0.5817\n",
      "Epoch: 0102, Loss: 0.5641\n",
      "Epoch: 0103, Loss: 0.5787\n",
      "Epoch: 0104, Loss: 0.5830\n",
      "Epoch: 0105, Loss: 0.5771\n",
      "Epoch: 0106, Loss: 0.5658\n",
      "Epoch: 0107, Loss: 0.5712\n",
      "Epoch: 0108, Loss: 0.5801\n",
      "Epoch: 0109, Loss: 0.5759\n",
      "Epoch: 0110, Loss: 0.5672\n",
      "Epoch: 0111, Loss: 0.5838\n",
      "Epoch: 0112, Loss: 0.5607\n",
      "Epoch: 0113, Loss: 0.5732\n",
      "Epoch: 0114, Loss: 0.5596\n",
      "Epoch: 0115, Loss: 0.5699\n",
      "Epoch: 0116, Loss: 0.5702\n",
      "Epoch: 0117, Loss: 0.5671\n",
      "Epoch: 0118, Loss: 0.5739\n",
      "Epoch: 0119, Loss: 0.5661\n",
      "Epoch: 0120, Loss: 0.5833\n",
      "Epoch: 0121, Loss: 0.5671\n",
      "Epoch: 0122, Loss: 0.5640\n",
      "Epoch: 0123, Loss: 0.5564\n",
      "Epoch: 0124, Loss: 0.5834\n",
      "Epoch: 0125, Loss: 0.5728\n",
      "Epoch: 0126, Loss: 0.5599\n",
      "Epoch: 0127, Loss: 0.5714\n",
      "Epoch: 0128, Loss: 0.5643\n",
      "Epoch: 0129, Loss: 0.5654\n",
      "Epoch: 0130, Loss: 0.5457\n",
      "Epoch: 0131, Loss: 0.5535\n",
      "Epoch: 0132, Loss: 0.5570\n",
      "Epoch: 0133, Loss: 0.5534\n",
      "Epoch: 0134, Loss: 0.5551\n",
      "Epoch: 0135, Loss: 0.5557\n",
      "Epoch: 0136, Loss: 0.5507\n",
      "Epoch: 0137, Loss: 0.5470\n",
      "Epoch: 0138, Loss: 0.5379\n",
      "Epoch: 0139, Loss: 0.5546\n",
      "Epoch: 0140, Loss: 0.5569\n",
      "Epoch: 0141, Loss: 0.5371\n",
      "Epoch: 0142, Loss: 0.5433\n",
      "Epoch: 0143, Loss: 0.5575\n",
      "Epoch: 0144, Loss: 0.5502\n",
      "Epoch: 0145, Loss: 0.5457\n",
      "Epoch: 0146, Loss: 0.5536\n",
      "Epoch: 0147, Loss: 0.5467\n",
      "Epoch: 0148, Loss: 0.5470\n",
      "Epoch: 0149, Loss: 0.5301\n",
      "Epoch: 0150, Loss: 0.5527\n",
      "Epoch: 0151, Loss: 0.5569\n",
      "Epoch: 0152, Loss: 0.5343\n",
      "Epoch: 0153, Loss: 0.5616\n",
      "Epoch: 0154, Loss: 0.5485\n",
      "Epoch: 0155, Loss: 0.5407\n",
      "Epoch: 0156, Loss: 0.5457\n",
      "Epoch: 0157, Loss: 0.5322\n",
      "Epoch: 0158, Loss: 0.5706\n",
      "Epoch: 0159, Loss: 0.5512\n",
      "Epoch: 0160, Loss: 0.5379\n",
      "Epoch: 0161, Loss: 0.5339\n",
      "Epoch: 0162, Loss: 0.5511\n",
      "Epoch: 0163, Loss: 0.5377\n",
      "Epoch: 0164, Loss: 0.5629\n",
      "Epoch: 0165, Loss: 0.5410\n",
      "Epoch: 0166, Loss: 0.5539\n",
      "Epoch: 0167, Loss: 0.5317\n",
      "Epoch: 0168, Loss: 0.5679\n",
      "Epoch: 0169, Loss: 0.5430\n",
      "Epoch: 0170, Loss: 0.5638\n",
      "Epoch: 0171, Loss: 0.5235\n",
      "Epoch: 0172, Loss: 0.5540\n",
      "Epoch: 0173, Loss: 0.5390\n",
      "Epoch: 0174, Loss: 0.5339\n",
      "Epoch: 0175, Loss: 0.5349\n",
      "Epoch: 0176, Loss: 0.5335\n",
      "Epoch: 0177, Loss: 0.5486\n",
      "Epoch: 0178, Loss: 0.5421\n",
      "Epoch: 0179, Loss: 0.5456\n",
      "Epoch: 0180, Loss: 0.5327\n",
      "Epoch: 0181, Loss: 0.5378\n",
      "Epoch: 0182, Loss: 0.5428\n",
      "Epoch: 0183, Loss: 0.5509\n",
      "Epoch: 0184, Loss: 0.5497\n",
      "Epoch: 0185, Loss: 0.5397\n",
      "Epoch: 0186, Loss: 0.5402\n",
      "Epoch: 0187, Loss: 0.5364\n",
      "Epoch: 0188, Loss: 0.5425\n",
      "Epoch: 0189, Loss: 0.5329\n",
      "Epoch: 0190, Loss: 0.5252\n",
      "Epoch: 0191, Loss: 0.5362\n",
      "Epoch: 0192, Loss: 0.5333\n",
      "Epoch: 0193, Loss: 0.5344\n",
      "Epoch: 0194, Loss: 0.5183\n",
      "Epoch: 0195, Loss: 0.5265\n",
      "Epoch: 0196, Loss: 0.5390\n",
      "Epoch: 0197, Loss: 0.5392\n",
      "Epoch: 0198, Loss: 0.5290\n",
      "Epoch: 0199, Loss: 0.5396\n",
      "Epoch: 0200, Loss: 0.5264\n",
      "Epoch: 0201, Loss: 0.5408\n",
      "Epoch: 0202, Loss: 0.5429\n",
      "Epoch: 0203, Loss: 0.5277\n",
      "Epoch: 0204, Loss: 0.5275\n",
      "Epoch: 0205, Loss: 0.5275\n",
      "Epoch: 0206, Loss: 0.5291\n",
      "Epoch: 0207, Loss: 0.5311\n",
      "Epoch: 0208, Loss: 0.5386\n",
      "Epoch: 0209, Loss: 0.5252\n",
      "Epoch: 0210, Loss: 0.5255\n",
      "Epoch: 0211, Loss: 0.5245\n",
      "Epoch: 0212, Loss: 0.5187\n",
      "Epoch: 0213, Loss: 0.5439\n",
      "Epoch: 0214, Loss: 0.5292\n",
      "Epoch: 0215, Loss: 0.5338\n",
      "Epoch: 0216, Loss: 0.5246\n",
      "Epoch: 0217, Loss: 0.5401\n",
      "Epoch: 0218, Loss: 0.5264\n",
      "Epoch: 0219, Loss: 0.5323\n",
      "Epoch: 0220, Loss: 0.5288\n",
      "Epoch: 0221, Loss: 0.5334\n",
      "Epoch: 0222, Loss: 0.5261\n",
      "Epoch: 0223, Loss: 0.5306\n",
      "Epoch: 0224, Loss: 0.5283\n",
      "Epoch: 0225, Loss: 0.5301\n",
      "Epoch: 0226, Loss: 0.5349\n",
      "Epoch: 0227, Loss: 0.5212\n",
      "Epoch: 0228, Loss: 0.5189\n",
      "Epoch: 0229, Loss: 0.5338\n",
      "Epoch: 0230, Loss: 0.5180\n",
      "Epoch: 0231, Loss: 0.5460\n",
      "Epoch: 0232, Loss: 0.5278\n",
      "Epoch: 0233, Loss: 0.5250\n",
      "Epoch: 0234, Loss: 0.5361\n",
      "Epoch: 0235, Loss: 0.5318\n",
      "Epoch: 0236, Loss: 0.5216\n",
      "Epoch: 0237, Loss: 0.5363\n",
      "Epoch: 0238, Loss: 0.5160\n",
      "Epoch: 0239, Loss: 0.5218\n",
      "Epoch: 0240, Loss: 0.5333\n",
      "Epoch: 0241, Loss: 0.5130\n",
      "Epoch: 0242, Loss: 0.5213\n",
      "Epoch: 0243, Loss: 0.5267\n",
      "Epoch: 0244, Loss: 0.5219\n",
      "Epoch: 0245, Loss: 0.5310\n",
      "Epoch: 0246, Loss: 0.5228\n",
      "Epoch: 0247, Loss: 0.5296\n",
      "Epoch: 0248, Loss: 0.5262\n",
      "Epoch: 0249, Loss: 0.5237\n",
      "Epoch: 0250, Loss: 0.5200\n",
      "Epoch: 0251, Loss: 0.5430\n",
      "Epoch: 0252, Loss: 0.5225\n",
      "Epoch: 0253, Loss: 0.5354\n",
      "Epoch: 0254, Loss: 0.5128\n",
      "Epoch: 0255, Loss: 0.5161\n",
      "Epoch: 0256, Loss: 0.5214\n",
      "Epoch: 0257, Loss: 0.5269\n",
      "Epoch: 0258, Loss: 0.5153\n",
      "Epoch: 0259, Loss: 0.5168\n",
      "Epoch: 0260, Loss: 0.5199\n",
      "Epoch: 0261, Loss: 0.5237\n",
      "Epoch: 0262, Loss: 0.5153\n",
      "Epoch: 0263, Loss: 0.5189\n",
      "Epoch: 0264, Loss: 0.5168\n",
      "Epoch: 0265, Loss: 0.5213\n",
      "Epoch: 0266, Loss: 0.5106\n",
      "Epoch: 0267, Loss: 0.5135\n",
      "Epoch: 0268, Loss: 0.5184\n",
      "Epoch: 0269, Loss: 0.5091\n",
      "Epoch: 0270, Loss: 0.5191\n",
      "Epoch: 0271, Loss: 0.5136\n",
      "Epoch: 0272, Loss: 0.5169\n",
      "Epoch: 0273, Loss: 0.5069\n",
      "Epoch: 0274, Loss: 0.5172\n",
      "Epoch: 0275, Loss: 0.5220\n",
      "Epoch: 0276, Loss: 0.5042\n",
      "Epoch: 0277, Loss: 0.5139\n",
      "Epoch: 0278, Loss: 0.5145\n",
      "Epoch: 0279, Loss: 0.5200\n",
      "Epoch: 0280, Loss: 0.5115\n",
      "Epoch: 0281, Loss: 0.5114\n",
      "Epoch: 0282, Loss: 0.5042\n",
      "Epoch: 0283, Loss: 0.5178\n",
      "Epoch: 0284, Loss: 0.5030\n",
      "Epoch: 0285, Loss: 0.5186\n",
      "Epoch: 0286, Loss: 0.5169\n",
      "Epoch: 0287, Loss: 0.5120\n",
      "Epoch: 0288, Loss: 0.5237\n",
      "Epoch: 0289, Loss: 0.5229\n",
      "Epoch: 0290, Loss: 0.5271\n",
      "Epoch: 0291, Loss: 0.5213\n",
      "Epoch: 0292, Loss: 0.5234\n",
      "Epoch: 0293, Loss: 0.5347\n",
      "Epoch: 0294, Loss: 0.5149\n",
      "Epoch: 0295, Loss: 0.5331\n",
      "Epoch: 0296, Loss: 0.5134\n",
      "Epoch: 0297, Loss: 0.5215\n",
      "Epoch: 0298, Loss: 0.5030\n",
      "Epoch: 0299, Loss: 0.5032\n",
      "Epoch: 0300, Loss: 0.5206\n",
      "Epoch: 0301, Loss: 0.5123\n",
      "Epoch: 0302, Loss: 0.5225\n",
      "Epoch: 0303, Loss: 0.5251\n",
      "Epoch: 0304, Loss: 0.4926\n",
      "Epoch: 0305, Loss: 0.5156\n",
      "Epoch: 0306, Loss: 0.5027\n",
      "Epoch: 0307, Loss: 0.5307\n",
      "Epoch: 0308, Loss: 0.5032\n",
      "Epoch: 0309, Loss: 0.5144\n",
      "Epoch: 0310, Loss: 0.4985\n",
      "Epoch: 0311, Loss: 0.5247\n",
      "Epoch: 0312, Loss: 0.5143\n",
      "Epoch: 0313, Loss: 0.5227\n",
      "Epoch: 0314, Loss: 0.5146\n",
      "Epoch: 0315, Loss: 0.5120\n",
      "Epoch: 0316, Loss: 0.5160\n",
      "Epoch: 0317, Loss: 0.5066\n",
      "Epoch: 0318, Loss: 0.5052\n",
      "Epoch: 0319, Loss: 0.5326\n",
      "Epoch: 0320, Loss: 0.5118\n",
      "Epoch: 0321, Loss: 0.5385\n",
      "Epoch: 0322, Loss: 0.5033\n",
      "Epoch: 0323, Loss: 0.5219\n",
      "Epoch: 0324, Loss: 0.4997\n",
      "Epoch: 0325, Loss: 0.5184\n",
      "Epoch: 0326, Loss: 0.5127\n",
      "Epoch: 0327, Loss: 0.5193\n",
      "Epoch: 0328, Loss: 0.5068\n",
      "Epoch: 0329, Loss: 0.5212\n",
      "Epoch: 0330, Loss: 0.5324\n",
      "Epoch: 0331, Loss: 0.4968\n",
      "Epoch: 0332, Loss: 0.5186\n",
      "Epoch: 0333, Loss: 0.5079\n",
      "Epoch: 0334, Loss: 0.5191\n",
      "Epoch: 0335, Loss: 0.5150\n",
      "Epoch: 0336, Loss: 0.5148\n",
      "Epoch: 0337, Loss: 0.5048\n",
      "Epoch: 0338, Loss: 0.5114\n",
      "Epoch: 0339, Loss: 0.4951\n",
      "Epoch: 0340, Loss: 0.5166\n",
      "Epoch: 0341, Loss: 0.4998\n",
      "Epoch: 0342, Loss: 0.5105\n",
      "Epoch: 0343, Loss: 0.5080\n",
      "Epoch: 0344, Loss: 0.5005\n",
      "Epoch: 0345, Loss: 0.5088\n",
      "Epoch: 0346, Loss: 0.4984\n",
      "Epoch: 0347, Loss: 0.5056\n",
      "Epoch: 0348, Loss: 0.5004\n",
      "Epoch: 0349, Loss: 0.5021\n",
      "Epoch: 0350, Loss: 0.5272\n",
      "Epoch: 0351, Loss: 0.5070\n",
      "Epoch: 0352, Loss: 0.4992\n",
      "Epoch: 0353, Loss: 0.4950\n",
      "Epoch: 0354, Loss: 0.5037\n",
      "Epoch: 0355, Loss: 0.5114\n",
      "Epoch: 0356, Loss: 0.4829\n",
      "Epoch: 0357, Loss: 0.5046\n",
      "Epoch: 0358, Loss: 0.4864\n",
      "Epoch: 0359, Loss: 0.5089\n",
      "Epoch: 0360, Loss: 0.4838\n",
      "Epoch: 0361, Loss: 0.4867\n",
      "Epoch: 0362, Loss: 0.4947\n",
      "Epoch: 0363, Loss: 0.4967\n",
      "Epoch: 0364, Loss: 0.4990\n",
      "Epoch: 0365, Loss: 0.4958\n",
      "Epoch: 0366, Loss: 0.4997\n",
      "Epoch: 0367, Loss: 0.4897\n",
      "Epoch: 0368, Loss: 0.4929\n",
      "Epoch: 0369, Loss: 0.4959\n",
      "Epoch: 0370, Loss: 0.4996\n",
      "Epoch: 0371, Loss: 0.5089\n",
      "Epoch: 0372, Loss: 0.5006\n",
      "Epoch: 0373, Loss: 0.4965\n",
      "Epoch: 0374, Loss: 0.4946\n",
      "Epoch: 0375, Loss: 0.4957\n",
      "Epoch: 0376, Loss: 0.4930\n",
      "Epoch: 0377, Loss: 0.4863\n",
      "Epoch: 0378, Loss: 0.4991\n",
      "Epoch: 0379, Loss: 0.4887\n",
      "Epoch: 0380, Loss: 0.5023\n",
      "Epoch: 0381, Loss: 0.5026\n",
      "Epoch: 0382, Loss: 0.4975\n",
      "Epoch: 0383, Loss: 0.4969\n",
      "Epoch: 0384, Loss: 0.4878\n",
      "Epoch: 0385, Loss: 0.4979\n",
      "Epoch: 0386, Loss: 0.4834\n",
      "Epoch: 0387, Loss: 0.4953\n",
      "Epoch: 0388, Loss: 0.5038\n",
      "Epoch: 0389, Loss: 0.5043\n",
      "Epoch: 0390, Loss: 0.4938\n",
      "Epoch: 0391, Loss: 0.4942\n",
      "Epoch: 0392, Loss: 0.4977\n",
      "Epoch: 0393, Loss: 0.5004\n",
      "Epoch: 0394, Loss: 0.4899\n",
      "Epoch: 0395, Loss: 0.4859\n",
      "Epoch: 0396, Loss: 0.4896\n",
      "Epoch: 0397, Loss: 0.5110\n",
      "Epoch: 0398, Loss: 0.5049\n",
      "Epoch: 0399, Loss: 0.5075\n",
      "Epoch: 0400, Loss: 0.4995\n",
      "Epoch: 0401, Loss: 0.4934\n",
      "Epoch: 0402, Loss: 0.4990\n",
      "Epoch: 0403, Loss: 0.4953\n",
      "Epoch: 0404, Loss: 0.5146\n",
      "Epoch: 0405, Loss: 0.4828\n",
      "Epoch: 0406, Loss: 0.4902\n",
      "Epoch: 0407, Loss: 0.4848\n",
      "Epoch: 0408, Loss: 0.4916\n",
      "Epoch: 0409, Loss: 0.4850\n",
      "Epoch: 0410, Loss: 0.4845\n",
      "Epoch: 0411, Loss: 0.4909\n",
      "Epoch: 0412, Loss: 0.4956\n",
      "Epoch: 0413, Loss: 0.4761\n",
      "Epoch: 0414, Loss: 0.4806\n",
      "Epoch: 0415, Loss: 0.4960\n",
      "Epoch: 0416, Loss: 0.4749\n",
      "Epoch: 0417, Loss: 0.4988\n",
      "Epoch: 0418, Loss: 0.5021\n",
      "Epoch: 0419, Loss: 0.4880\n",
      "Epoch: 0420, Loss: 0.4936\n",
      "Epoch: 0421, Loss: 0.4816\n",
      "Epoch: 0422, Loss: 0.4738\n",
      "Epoch: 0423, Loss: 0.4861\n",
      "Epoch: 0424, Loss: 0.4810\n",
      "Epoch: 0425, Loss: 0.4927\n",
      "Epoch: 0426, Loss: 0.4840\n",
      "Epoch: 0427, Loss: 0.4787\n",
      "Epoch: 0428, Loss: 0.4765\n",
      "Epoch: 0429, Loss: 0.4962\n",
      "Epoch: 0430, Loss: 0.4888\n",
      "Epoch: 0431, Loss: 0.4773\n",
      "Epoch: 0432, Loss: 0.4926\n",
      "Epoch: 0433, Loss: 0.4845\n",
      "Epoch: 0434, Loss: 0.4893\n",
      "Epoch: 0435, Loss: 0.4875\n",
      "Epoch: 0436, Loss: 0.5034\n",
      "Epoch: 0437, Loss: 0.4939\n",
      "Epoch: 0438, Loss: 0.4845\n",
      "Epoch: 0439, Loss: 0.4821\n",
      "Epoch: 0440, Loss: 0.4856\n",
      "Epoch: 0441, Loss: 0.4901\n",
      "Epoch: 0442, Loss: 0.4643\n",
      "Epoch: 0443, Loss: 0.4744\n",
      "Epoch: 0444, Loss: 0.4950\n",
      "Epoch: 0445, Loss: 0.4954\n",
      "Epoch: 0446, Loss: 0.4927\n",
      "Epoch: 0447, Loss: 0.4760\n",
      "Epoch: 0448, Loss: 0.4860\n",
      "Epoch: 0449, Loss: 0.4923\n",
      "Epoch: 0450, Loss: 0.4836\n",
      "Epoch: 0451, Loss: 0.4865\n",
      "Epoch: 0452, Loss: 0.4887\n",
      "Epoch: 0453, Loss: 0.4886\n",
      "Epoch: 0454, Loss: 0.4908\n",
      "Epoch: 0455, Loss: 0.4822\n",
      "Epoch: 0456, Loss: 0.4933\n",
      "Epoch: 0457, Loss: 0.4818\n",
      "Epoch: 0458, Loss: 0.4793\n",
      "Epoch: 0459, Loss: 0.4808\n",
      "Epoch: 0460, Loss: 0.4899\n",
      "Epoch: 0461, Loss: 0.4768\n",
      "Epoch: 0462, Loss: 0.4802\n",
      "Epoch: 0463, Loss: 0.4977\n",
      "Epoch: 0464, Loss: 0.4781\n",
      "Epoch: 0465, Loss: 0.4785\n",
      "Epoch: 0466, Loss: 0.4789\n",
      "Epoch: 0467, Loss: 0.4821\n",
      "Epoch: 0468, Loss: 0.4849\n",
      "Epoch: 0469, Loss: 0.4770\n",
      "Epoch: 0470, Loss: 0.4797\n",
      "Epoch: 0471, Loss: 0.4771\n",
      "Epoch: 0472, Loss: 0.4831\n",
      "Epoch: 0473, Loss: 0.4816\n",
      "Epoch: 0474, Loss: 0.4623\n",
      "Epoch: 0475, Loss: 0.4875\n",
      "Epoch: 0476, Loss: 0.4594\n",
      "Epoch: 0477, Loss: 0.4874\n",
      "Epoch: 0478, Loss: 0.4866\n",
      "Epoch: 0479, Loss: 0.4891\n",
      "Epoch: 0480, Loss: 0.4791\n",
      "Epoch: 0481, Loss: 0.4676\n",
      "Epoch: 0482, Loss: 0.4731\n",
      "Epoch: 0483, Loss: 0.4725\n",
      "Epoch: 0484, Loss: 0.4907\n",
      "Epoch: 0485, Loss: 0.4757\n",
      "Epoch: 0486, Loss: 0.4827\n",
      "Epoch: 0487, Loss: 0.4880\n",
      "Epoch: 0488, Loss: 0.4876\n",
      "Epoch: 0489, Loss: 0.4854\n",
      "Epoch: 0490, Loss: 0.4843\n",
      "Epoch: 0491, Loss: 0.4788\n",
      "Epoch: 0492, Loss: 0.4824\n",
      "Epoch: 0493, Loss: 0.4672\n",
      "Epoch: 0494, Loss: 0.4733\n",
      "Epoch: 0495, Loss: 0.4783\n",
      "Epoch: 0496, Loss: 0.4806\n",
      "Epoch: 0497, Loss: 0.4746\n",
      "Epoch: 0498, Loss: 0.4645\n",
      "Epoch: 0499, Loss: 0.4774\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        c1_channels = 128\n",
    "        c2_channels = 256\n",
    "        lin1_channels = 128\n",
    "        lin2_channels = 64\n",
    "        self.conv1 = GCNConv(in_channels,  c1_channels)\n",
    "        self.conv2 = GCNConv(c1_channels, c2_channels)\n",
    "        self.lin1 = Linear(c2_channels, lin1_channels)\n",
    "        self.lin2 = Linear(lin1_channels, lin2_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index).relu()\n",
    "        h = self.conv2(h, edge_index).relu()\n",
    "        h = self.lin1(h).relu()\n",
    "        h = self.lin2(h)\n",
    "        return h\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "def fit(train_data, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def test(data):\n",
    "#     model.eval()\n",
    "#     z = model.encode(data.x, data.edge_index)\n",
    "#     out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "#     return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "def train_loop(train_data):\n",
    "    model = Net(train_data.x.shape[1]).to(device)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    for epoch in range(1, 500):\n",
    "        loss = fit(train_data, model, optimizer, criterion)\n",
    "        print(f'Epoch: {epoch:04d}, Loss: {loss:.4f}')\n",
    "    return model\n",
    "\n",
    "model1 = train_loop(train_data1)\n",
    "model2 = train_loop(train_data2)\n",
    "model3 = train_loop(train_data3)\n",
    "# z = model.encode(test_data.x, test_data.edge_index)\n",
    "# final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_data, model):\n",
    "    z = model.encode(test_data.x, test_data.edge_index)\n",
    "    test_pred = torch.sigmoid(model.decode(z, test_data.edge_label_index))\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_pred = predict(test_data1, model1)\n",
    "test2_pred = predict(test_data2, model2)\n",
    "test3_pred = predict(test_data3, model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'id': df_test1['id'],\n",
    "    'prob': test1_pred.tolist()\n",
    "}).to_csv('test1_upload.csv', index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'id': df_test2['id'],\n",
    "    'prob': test2_pred.tolist()\n",
    "}).to_csv('test2_upload.csv', index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'id': df_test3['id'],\n",
    "    'prob': test3_pred.tolist()\n",
    "}).to_csv('test3_upload.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterhub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
